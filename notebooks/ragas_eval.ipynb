{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_testset = pd.read_csv('../ragas_openai_gpt4o-pt-corrigido.csv')\n",
    "df_testset.rename(columns = {'reference': 'response'}, inplace = True)\n",
    "df_testset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "custom_llm = BedrockLLM(\n",
    "    region='us-east-1',\n",
    "    model_id=model_id,  # ARN like 'arn:aws:bedrock:...' obtained via provisioning the custom model\n",
    "    model_kwargs={\"temperature\": 0.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Você é um servidor da UFRN responsável por instruir alunos sobre questões acadêmicas do regulamento dos cursos de graduação.\n",
    "Você deve responder a resposta correta baseada na questão e contexto abaixo. Por favor, siga as instruções:\n",
    "\n",
    "1. Pergunta: {pergunta}\n",
    "\n",
    "2. Contexto: {contexto}\n",
    "\n",
    "3. Instruções:\n",
    "    - Analise cuidadosamente a questão e o contexto fornecido.\n",
    "    - Formule uma resposta abrangente e precisa baseada apenas nas informações fornecidas no contexto.\n",
    "    - Certifique-se de que sua resposta aborda diretamente a pergunta.\n",
    "    - Inclua todas as informações relevantes do contexto, mas não adicione nenhum conhecimento externo.\n",
    "    - Se o contexto não contiver informações suficientes para responder completamente à pergunta, declare isso claramente e forneça a melhor resposta parcial possível.\n",
    "    - Use um tom formal e objetivo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "responses = []\n",
    "\n",
    "for _, row in df_testset.iterrows():\n",
    "    row.to_dict()\n",
    "    prompt = prompt_template.format(pergunta=row['user_input'], contexto=row['reference_contexts'])\n",
    "    try:\n",
    "        response = custom_llm.invoke(prompt)\n",
    "        print(response)\n",
    "        responses.append(response)\n",
    "    except:\n",
    "        sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset['reference'] = pd.Series(responses)\n",
    "df_testset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset.to_csv('../dataset_intermediario.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_testset = pd.read_csv('../dataset_intermediario.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def pandas_to_ragas(df):\n",
    "    '''\n",
    "    Converts a Pandas DataFrame into a Ragas-compatible dataset\n",
    "    \n",
    "    Inputs:\n",
    "        - df (Pandas DataFrame): The input DataFrame to be converted\n",
    "        \n",
    "    Returns:\n",
    "        - ragas_testset (Hugging Face Dataset): A Hugging Face dataset compatible with the Ragas framework\n",
    "    '''\n",
    "    # Ensure all text columns are strings and handle NaN values\n",
    "    text_columns = df.columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "        \n",
    "    # Convert 'contexts' to a list of lists\n",
    "    df['reference_contexts'] = df['reference_contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n",
    "    # df['retrieved_contexts'] = df['retrieved_contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n",
    "    \n",
    "    # Converting the DataFrame to a dictionary\n",
    "    data_dict = df.to_dict('list')\n",
    "    \n",
    "    # Loading the dictionary as a Hugging Face dataset\n",
    "    ragas_testset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    return ragas_testset\n",
    "\n",
    "eval_dataset = pandas_to_ragas(df_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.metrics import NonLLMContextRecall\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    # LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    # Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[df_result['factual_correctness'] > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics._string import NonLLMStringSimilarity\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in India.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\"\n",
    ")\n",
    "\n",
    "scorer = NonLLMStringSimilarity()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
