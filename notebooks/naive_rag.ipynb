{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqqU langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta.llama3-8b-instruct-v1:0'\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.07,  # <-- Super slow! We can only make a request once every 10 seconds!!\n",
    "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
    "    # max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\"temperature\": 0},\n",
    "    rate_limiter=rate_limiter\n",
    "    )\n",
    "\n",
    "embeddings = BedrockEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    location=os.environ[\"VECTOR_STORE_URL\"],\n",
    "    api_key=os.environ[\"VECTOR_STORE_API_KEY\"]\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"regulamento-semantic\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Você é um assistente da UFRN responsável por instruir alunos sobre questões acadêmicas do regulamento dos cursos de graduação.\n",
    "Você deve responder a resposta correta baseada na questão e contexto abaixo. Por favor, siga as instruções:\n",
    "\n",
    "1. Pergunta: {pergunta}\n",
    "\n",
    "2. Contexto: {contexto}\n",
    "\n",
    "3. Instruções:\n",
    "    - Analise cuidadosamente a questão e o contexto fornecido.\n",
    "    - Formule uma resposta abrangente e precisa baseada apenas nas informações fornecidas no contexto.\n",
    "    - Certifique-se de que sua resposta aborda diretamente a pergunta.\n",
    "    - Inclua todas as informações relevantes do contexto, mas não adicione nenhum conhecimento externo.\n",
    "    - Se o contexto não contiver informações suficientes para responder completamente à pergunta, declare isso claramente e forneça a melhor resposta parcial possível.\n",
    "    - Use um tom formal e objetivo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/dataset_potiguana.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "retrieved_contexts = []\n",
    "responses = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    query = row['user_input']\n",
    "    context_docs = vector_store.similarity_search(query, k=2)\n",
    "    contexts = [c.page_content for c in context_docs]\n",
    "    prompt = prompt_template.format(contexto=contexts, pergunta=query)\n",
    "    base_message = llm.invoke(prompt)\n",
    "    response = base_message.content\n",
    "\n",
    "    retrieved_contexts.append(contexts)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'] = pd.Series(responses)\n",
    "df['retrieved_contexts'] = pd.Series(retrieved_contexts)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../naive_rag_llama3-8b_k2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def pandas_to_ragas(df):\n",
    "    '''\n",
    "    Converts a Pandas DataFrame into a Ragas-compatible dataset\n",
    "    \n",
    "    Inputs:\n",
    "        - df (Pandas DataFrame): The input DataFrame to be converted\n",
    "        \n",
    "    Returns:\n",
    "        - ragas_testset (Hugging Face Dataset): A Hugging Face dataset compatible with the Ragas framework\n",
    "    '''\n",
    "    # Ensure all text columns are strings and handle NaN values\n",
    "    text_columns = df.columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "        \n",
    "    # Convert 'contexts' to a list of lists\n",
    "    df['reference_contexts'] = df['reference_contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n",
    "    df['retrieved_contexts'] = df['retrieved_contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n",
    "    \n",
    "    # Converting the DataFrame to a dictionary\n",
    "    data_dict = df.to_dict('list')\n",
    "    \n",
    "    # Loading the dictionary as a Hugging Face dataset\n",
    "    ragas_testset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    return ragas_testset\n",
    "\n",
    "eval_dataset = pandas_to_ragas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    # ContextPrecision,\n",
    "    LLMContextRecall,\n",
    "    NonLLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    ResponseRelevancy,\n",
    "    Faithfulness,\n",
    "    SemanticSimilarity,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    # ContextPrecision(llm=evaluator_llm),\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    NonLLMContextRecall(),\n",
    "    ContextEntityRecall(llm=evaluator_llm),\n",
    "    ResponseRelevancy(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('../naive_rag_llama3-8b_k2_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
