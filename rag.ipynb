{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqqU langchain pypdf boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(\"docs\")\n",
    "    return document_loader.load()\n",
    "\n",
    "len(load_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BedrockEmbeddings(client=<botocore.client.BedrockRuntime object at 0x737011303990>, region_name='us-east-1', credentials_profile_name='default', model_id='amazon.titan-embed-text-v1', model_kwargs=None, endpoint_url=None, normalize=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "get_embedding_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='llama3', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    return embeddings\n",
    "\n",
    "get_embedding_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=\"./chroma\", embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "üëâ Adding new documents: 267\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Nesse contexto, os requisitos de turma espec√≠fica s√£o:\n",
      "\n",
      "* A estudante n√£o pode ter sido suspensa do programa;\n",
      "* O estudante esteve realizando mobilidade acad√™mica em outra institui√ß√£o e n√£o inclui per√≠odos letivos contados no perfil inicial;\n",
      "* A carga hor√°ria m√≠nima (CHM) e a dura√ß√£o padr√£o (DP) devem ser atendidas para integraliza√ß√£o da estrutura curricular do estudante, de acordo com o Projeto Pedag√≥gico do Curso;\n",
      "* O estudante regular deve estar vinculado ao curso equivalente ou ao curso correlato na institui√ß√£o de origem.\n",
      "\n",
      "Portanto, os requisitos de turma espec√≠fica s√£o: n√£o suspens√£o do programa, realiza√ß√£o de mobilidade acad√™mica em outra institui√ß√£o, integraliza√ß√£o da estrutura curricular e v√≠nculo com o curso equivalente ou correlato.\n",
      "Sources: ['docs/regulamento-dos-cursos-de-graduacao-da-UFRN-2024.pdf:12:2', 'docs/regulamento-dos-cursos-de-graduacao-da-UFRN-2024.pdf:76:3', 'docs/regulamento-dos-cursos-de-graduacao-da-UFRN-2024.pdf:23:3', 'docs/regulamento-dos-cursos-de-graduacao-da-UFRN-2024.pdf:39:3', 'docs/regulamento-dos-cursos-de-graduacao-da-UFRN-2024.pdf:56:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nesse contexto, os requisitos de turma espec√≠fica s√£o:\\n\\n* A estudante n√£o pode ter sido suspensa do programa;\\n* O estudante esteve realizando mobilidade acad√™mica em outra institui√ß√£o e n√£o inclui per√≠odos letivos contados no perfil inicial;\\n* A carga hor√°ria m√≠nima (CHM) e a dura√ß√£o padr√£o (DP) devem ser atendidas para integraliza√ß√£o da estrutura curricular do estudante, de acordo com o Projeto Pedag√≥gico do Curso;\\n* O estudante regular deve estar vinculado ao curso equivalente ou ao curso correlato na institui√ß√£o de origem.\\n\\nPortanto, os requisitos de turma espec√≠fica s√£o: n√£o suspens√£o do programa, realiza√ß√£o de mobilidade acad√™mica em outra institui√ß√£o, integraliza√ß√£o da estrutura curricular e v√≠nculo com o curso equivalente ou correlato.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"Qual os requisitos de turma espec√≠fica?\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Responda a quest√£o apenas baseada nesse contexto:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Responda a quest√£o baseada no contexto acima: {question}\n",
    "\"\"\"\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma\", embedding_function=get_embedding_function())\n",
    "results = db.similarity_search_with_score(query, k=5)\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "model = Ollama(model=\"llama3\", temperature=.7)\n",
    "response_text = model.invoke(prompt)\n",
    "\n",
    "sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "print(formatted_response)\n",
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
