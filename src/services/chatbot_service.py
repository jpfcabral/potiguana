import random
from time import sleep

from langchain_aws import ChatBedrock
from langchain_core.messages import BaseMessage
from loguru import logger

from agents.agent import agent
from services.telegram_service import DynamoDBRepository

random.seed(21)


class ChatbotService:
    """
    Service for interacting with a chatbot model hosted on AWS Bedrock.

    This service handles context retrieval, reranking, and communication with a language model
    to generate responses based on user queries.
    """

    def __init__(
        self,
        repository: DynamoDBRepository = DynamoDBRepository(),
    ):
        """
        Initializes the ChatbotService with the specified model.

        Args:
            model_name (str): The identifier of the LLM model to use. Defaults to 'amazon.nova-micro-v1:0'.
        """
        self.repository = repository

    def answer(self, query: str):
        """
        Provides an answer to a user query using retrieved and reranked contexts.

        Args:
            query (str): The user's input question or query.

        Returns:
            dict: The response generated by the language model, represented as a dictionary.
        """

        output = agent.invoke({"messages": [("user", query)]})

        response: BaseMessage = output["messages"][-1]

        response_dict = response.dict()
        self.repository.insert(data=response_dict, table_name="responses")

        logger.debug(f"LLM Response: {response}")

        return response_dict

    def invoke_llm_with_backoff(self, llm: ChatBedrock, prompt, max_retries=5):
        """
        Invokes the language model with exponential backoff for handling throttling errors.

        Args:
            llm (ChatBedrock): The language model client.
            prompt (str): The prompt to send to the model.
            max_retries (int): Maximum number of retries for throttling errors. Defaults to 5.

        Returns:
            BaseMessage: The response from the language model.

        Raises:
            Exception: If the maximum number of retries is reached.
        """
        retries = 0
        while retries < max_retries:
            try:
                return llm.invoke(prompt)
            except Exception as exc:
                logger.warning(exc)
                retries += 1
                wait_time = random.uniform(  # nosec
                    2**retries, 2**retries + 5
                )  # Exponential backoff
                logger.warning(
                    f"Throttling error. Retrying in {wait_time:.2f} seconds..."
                )
                sleep(wait_time)

        raise Exception("Max retries reached, could not invoke model.")
