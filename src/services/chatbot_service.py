import random
from time import sleep

from langchain_aws import ChatBedrock
from langchain_core.messages import BaseMessage
from loguru import logger

from agents.agent import agent
from services.telegram_service import DynamoDBRepository

random.seed(21)


class ChatbotService:
    """
    Service for interacting with a chatbot model hosted on AWS Bedrock.

    This service handles context retrieval, reranking, and communication with a language model
    to generate responses based on user queries.
    """

    def __init__(
        self,
        repository: DynamoDBRepository = DynamoDBRepository(),
    ):
        """
        Initializes the ChatbotService with the specified model.

        Args:
            model_name (str): The identifier of the LLM model to use. Defaults to 'amazon.nova-micro-v1:0'.
        """
        self.repository = repository

    def answer(self, query: str, chat_id: str = None) -> dict:
        """
        Provides an answer to a user query using retrieved and reranked contexts.

        Args:
            query (str): The user's input question or query.

        Returns:
            dict: The response generated by the language model, represented as a dictionary.
        """
        logger.debug(f"User query: {query}, chat_id: {chat_id}")

        # Check if the query is valid
        if not self._is_safe(query):
            return {"content": "NÃ£o consegui processar sua pergunta"}

        config = {"configurable": {"thread_id": chat_id}}
        output = agent.invoke({"messages": [("user", query)]}, config=config)

        response: BaseMessage = output["messages"][-1]

        response_dict = response.dict()
        self.repository.insert(data=response_dict, table_name="responses")

        logger.debug(f"LLM Response: {response}")

        return response_dict

    def invoke_llm_with_backoff(self, llm: ChatBedrock, prompt, max_retries=5):
        """
        Invokes the language model with exponential backoff for handling throttling errors.

        Args:
            llm (ChatBedrock): The language model client.
            prompt (str): The prompt to send to the model.
            max_retries (int): Maximum number of retries for throttling errors. Defaults to 5.

        Returns:
            BaseMessage: The response from the language model.

        Raises:
            Exception: If the maximum number of retries is reached.
        """
        retries = 0
        while retries < max_retries:
            try:
                return llm.invoke(prompt)
            except Exception as exc:
                logger.warning(exc)
                retries += 1
                wait_time = random.uniform(  # nosec
                    2**retries, 2**retries + 5
                )  # Exponential backoff
                logger.warning(
                    f"Throttling error. Retrying in {wait_time:.2f} seconds..."
                )
                sleep(wait_time)

        raise Exception("Max retries reached, could not invoke model.")

    def _is_safe(self, query: str) -> bool:
        """
        Checks if the query is valid.

        Args:
            query (str): The query to check.

        Returns:
            bool: True if the query is valid, False otherwise.
        """
        # Check prompt injection
        if "<script>" in query or "</script>" in query:
            logger.warning("Prompt injection detected")
            return False

        # Check query lenght
        if len(query) > 500:
            logger.warning("Query too long")
            return False

        # Check empty query
        if not query.strip():
            logger.warning("Empty query")
            return False

        return True
